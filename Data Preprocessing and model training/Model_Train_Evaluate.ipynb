{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4e6ea1c-e49c-4359-9ef2-fe5069ee99ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import spacy\n",
    "from spacy.tokens import DocBin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cb0bc56-7c99-4c71-ba40-14b7e417dbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "932ca9af-2e60-44c4-af7e-a459d43cbe31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4m[i] Saving to output directory: output\u001b[0mCPU times: total: 2min 51s\n",
      "Wall time: 8h 15min 7s\n",
      "\n",
      "\u001b[38;5;4m[i] Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m[+] Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4m[i] Pipeline: ['transformer', 'ner']\u001b[0m\n",
      "\u001b[38;5;4m[i] Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  -------------  --------  ------  ------  ------  ------\n",
      "  0       0         914.41   1115.57    4.16    2.32   20.33    0.04\n",
      "  0     200       41502.85  60611.17   71.43   71.70   71.16    0.71\n",
      "  0     400        4479.98  11685.33   79.73   77.60   81.99    0.80\n",
      "  0     600        3308.81   9522.61   80.47   79.27   81.71    0.80\n",
      "  0     800        2892.84   8893.83   79.36   77.64   81.16    0.79\n",
      "  0    1000        3073.81   8886.11   83.08   84.59   81.63    0.83\n",
      "  0    1200        2882.57   8807.70   83.52   84.97   82.12    0.84\n",
      "  0    1400        2439.14   8079.28   83.63   83.49   83.77    0.84\n",
      "  0    1600        2595.17   8387.58   83.91   84.71   83.12    0.84\n",
      "  0    1800        2447.72   8049.93   83.91   85.41   82.47    0.84\n",
      "  0    2000        2577.56   8355.34   83.44   83.14   83.74    0.83\n",
      "  0    2200        2386.49   7969.08   84.63   83.92   85.36    0.85\n",
      "  0    2400        2202.20   7614.61   85.06   87.08   83.12    0.85\n",
      "  0    2600        2424.31   7859.22   85.10   86.45   83.78    0.85\n",
      "  0    2800        2199.77   7513.89   85.31   86.47   84.17    0.85\n",
      "  0    3000        2215.99   7723.45   85.60   85.17   86.04    0.86\n",
      "  0    3200        2049.97   7337.38   86.19   87.34   85.07    0.86\n",
      "  0    3400        2085.49   7428.08   86.40   86.47   86.33    0.86\n",
      "  0    3600        2072.24   7409.82   85.42   86.87   84.01    0.85\n",
      "  1    3800        1992.78   7159.01   86.61   85.38   87.87    0.87\n",
      "  1    4000        1893.03   6927.02   86.50   88.71   84.40    0.87\n",
      "  1    4200        1945.51   6907.78   86.50   85.75   87.26    0.86\n",
      "  1    4400        1948.71   6917.81   86.98   88.06   85.92    0.87\n",
      "  1    4600        1847.02   6843.44   87.31   87.59   87.03    0.87\n",
      "  1    4800        1919.93   6888.45   87.39   87.41   87.36    0.87\n",
      "  1    5000        1859.25   6760.32   87.77   88.07   87.47    0.88\n",
      "  1    5200        1775.58   6652.63   87.91   88.16   87.66    0.88\n",
      "  1    5400        1841.86   6716.87   88.23   86.96   89.53    0.88\n",
      "  1    5600        1857.90   6811.37   87.83   87.65   88.02    0.88\n",
      "  1    5800        1796.63   6587.25   88.10   88.96   87.24    0.88\n",
      "  1    6000        1762.01   6474.91   87.99   85.82   90.27    0.88\n",
      "  1    6200        1703.09   6464.36   88.46   88.08   88.85    0.88\n",
      "  1    6400        1636.55   6326.45   88.34   86.81   89.94    0.88\n",
      "  1    6600        1702.51   6517.23   88.54   89.58   87.52    0.89\n",
      "  1    6800        1656.42   6367.32   88.21   90.22   86.28    0.88\n",
      "  1    7000        1687.28   6433.76   89.03   88.68   89.39    0.89\n",
      "  1    7200        1631.84   6303.72   88.85   88.73   88.97    0.89\n",
      "  1    7400        1602.07   6151.51   88.77   90.32   87.28    0.89\n",
      "  2    7600        1586.48   6108.56   87.81   88.56   87.07    0.88\n",
      "  2    7800        1507.55   5888.24   89.26   87.97   90.58    0.89\n",
      "  2    8000        1612.39   6047.87   89.02   87.96   90.11    0.89\n",
      "  2    8200        1561.00   5974.59   88.74   91.00   86.60    0.89\n",
      "  2    8400        1534.33   5963.79   89.09   91.25   87.04    0.89\n",
      "  2    8600        1545.76   6017.74   89.26   90.73   87.84    0.89\n",
      "  2    8800        1556.07   5892.86   89.45   89.80   89.10    0.89\n",
      "  2    9000        1518.15   5921.54   88.88   89.98   87.80    0.89\n",
      "  2    9200        1488.12   5844.53   89.44   90.61   88.29    0.89\n",
      "  2    9400        1459.09   5750.27   89.84   88.26   91.48    0.90\n",
      "  2    9600        1486.23   5809.82   89.62   88.41   90.86    0.90\n",
      "  2    9800        1500.35   5838.28   89.19   90.88   87.57    0.89\n",
      "  2   10000        1436.46   5697.17   89.54   89.29   89.78    0.90\n",
      "  2   10200        1479.78   5799.90   89.57   88.69   90.47    0.90\n",
      "  2   10400        1443.30   5695.00   89.64   89.47   89.81    0.90\n",
      "  2   10600        1447.57   5775.00   89.62   91.99   87.37    0.90\n",
      "  2   10800        1420.74   5647.35   89.74   89.62   89.86    0.90\n",
      "  2   11000        1409.64   5660.11   90.05   89.26   90.85    0.90\n",
      "  2   11200        1398.34   5566.32   89.79   89.90   89.68    0.90\n",
      "  3   11400        1423.26   5689.08   89.78   89.59   89.98    0.90\n",
      "  3   11600        1379.38   5518.26   89.60   91.25   88.00    0.90\n",
      "  3   11800        1369.49   5416.68   89.70   91.16   88.29    0.90\n",
      "  3   12000        1388.91   5616.19   89.82   92.18   87.59    0.90\n",
      "  3   12200        1359.46   5433.50   89.70   91.12   88.31    0.90\n",
      "  3   12400        1334.13   5404.53   90.26   88.81   91.75    0.90\n",
      "  3   12600        1342.46   5460.76   90.08   90.44   89.72    0.90\n",
      "  3   12800        1379.92   5514.25   89.95   87.48   92.57    0.90\n",
      "  3   13000        1307.88   5314.73   90.20   89.20   91.23    0.90\n",
      "  3   13200        1339.80   5403.88   90.20   89.89   90.52    0.90\n",
      "  3   13400        1332.69   5461.67   90.34   89.98   90.69    0.90\n",
      "  3   13600        1329.02   5390.69   90.09   91.62   88.60    0.90\n",
      "  3   13800        1346.78   5405.64   89.91   91.47   88.41    0.90\n",
      "  3   14000        1308.95   5323.50   90.27   91.32   89.23    0.90\n",
      "  3   14200        1327.73   5362.64   90.26   88.90   91.66    0.90\n",
      "  3   14400        1336.26   5408.36   90.06   92.33   87.89    0.90\n",
      "  3   14600        1308.47   5275.55   90.19   90.43   89.94    0.90\n",
      "  3   14800        1263.57   5271.14   90.42   90.59   90.26    0.90\n",
      "  3   15000        1307.61   5429.19   90.30   92.04   88.63    0.90\n",
      "  4   15200        1256.44   5207.89   90.64   88.90   92.45    0.91\n",
      "  4   15400        1281.15   5203.64   90.32   91.44   89.23    0.90\n",
      "  4   15600        1264.38   5144.05   90.06   89.95   90.16    0.90\n",
      "  4   15800        1267.63   5179.93   90.48   90.22   90.73    0.90\n",
      "  4   16000        1258.82   5204.16   90.28   91.74   88.87    0.90\n",
      "  4   16200        1255.86   5188.81   90.62   89.76   91.50    0.91\n",
      "  4   16400        1258.72   5216.15   90.33   91.76   88.94    0.90\n",
      "  4   16600        1259.85   5125.28   90.44   90.66   90.23    0.90\n",
      "  4   16800        1239.66   5096.11   90.38   90.89   89.88    0.90\n",
      "\u001b[38;5;2m[+] Saved pipeline to output directory\u001b[0m\n",
      "output\\model-last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python -m spacy train config.cfg --output ./output --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e7ab62-9061-49ab-ab2c-36a48fcfbf8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d1c402-48c3-4543-ac1b-460cb97cd882",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
